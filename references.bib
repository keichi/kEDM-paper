@article{VanBerkel2020,
abstract = {Researchers in Human-Computer Interaction typically rely on experiments to assess the causal effects of experimental conditions on variables of interest. Although this classic approach can be very useful, it offers little help in tackling questions of causality in the kind of data that are increasingly common in HCI–capturing user behavior ‘in the wild.' To analyze such data, model-based regressions such as cross-lagged panel models or vector autoregressions can be used, but these require parametric assumptions about the structural form of effects among the variables. To overcome some of the limitations associated with experiments and model-based regressions, we adopt and extend ‘empirical dynamic modelling' methods from ecology that lend themselves to conceptualizing multiple users' behavior as complex nonlinear dynamical systems. Extending a method known as ‘convergent cross mapping' or CCM, we show how to make causal inferences that do not rely on experimental manipulations or model-based regressions and, by virtue of being non-parametric, can accommodate data emanating from complex nonlinear dynamical systems. By using this approach for multiple users, which we call ‘multiple convergent cross mapping' or MCCM, researchers can achieve a better understanding of the interactions between users and technology–by distinguishing causality from correlation–in real-world settings.},
author = {van Berkel, Niels and Dennis, Simon and Zyphur, Michael and Li, Jinjing and Heathcote, Andrew and Kostakos, Vassilis},
doi = {10.1080/07370024.2020.1715221},
file = {:Users/keichi/Documents/Mendeley Desktop/hci20.pdf:pdf},
issn = {07370024},
journal = {Human-Computer Interaction},
keywords = {Ubicomp,causality,interaction,mobile,performance},
mendeley-groups = {kEDM},
pages = {1--27},
publisher = {Taylor {\&} Francis},
title = {Modeling interaction as a complex system},
year = {2020}
}
@article{Ma2014,
abstract = {Quantifying causality between variables from observed time series data is of great importance in various disciplines but also a challenging task, especially when the observed data are short. Unlike the conventional methods, we find it possible to detect causality only with very short time series data, based on embedding theory of an attractor for nonlinear dynamics. Specifically, we first show that measuring the smoothness of a cross map between two observed variables can be used to detect a causal relation. Then, we provide a very effective algorithm to computationally evaluate the smoothness of the cross map, or ''Cross Map Smoothness'' (CMS), and thus to infer the causality, which can achieve high accuracy even with very short time series data. Analysis of both mathematical models from various benchmarks and real data from biological systems validates our method.},
author = {Ma, Huanfei and Aihara, Kazuyuki and Chen, Luonan},
doi = {10.1038/srep07464},
file = {:Users/keichi/Documents/Mendeley Desktop/srep07464.pdf:pdf},
issn = {20452322},
journal = {Scientific Reports},
mendeley-groups = {kEDM},
pages = {1--10},
pmid = {25501646},
title = {Detecting causality from nonlinear dynamics with short-term time series},
volume = {4},
year = {2014}
}
@article{Demeshko2019,
abstract = {Performance portability on heterogeneous high-performance computing (HPC) systems is a major challenge faced today by code developers: parallel code needs to be executed correctly as well as with high performance on machines with different architectures, operating systems, and software libraries. The finite element method (FEM) is a popular and flexible method for discretizing partial differential equations arising in a wide variety of scientific, engineering, and industrial applications that require HPC. This article presents some preliminary results pertaining to our development of a performance portable implementation of the FEM-based Albany code. Performance portability is achieved using the Kokkos library. We present performance results for the Aeras global atmosphere dynamical core module in Albany. Numerical experiments show that our single code implementation gives reasonable performance across three multicore/many-core architectures: NVIDIA General Processing Units (GPU's), Intel Xeon Phis, and multicore CPUs.},
author = {Demeshko, Irina and Watkins, Jerry and Tezaur, Irina K. and Guba, Oksana and Spotz, William F. and Salinger, Andrew G. and Pawlowski, Roger P. and Heroux, Michael A.},
doi = {10.1177/1094342017749957},
file = {:Users/keichi/Documents/Mendeley Desktop/Demeshko{\_}et{\_}al{\_}IJHPCA{\_}Albany{\_}Kokkos{\_}paper{\_}2018.pdf:pdf},
isbn = {1094342017},
issn = {17412846},
journal = {International Journal of High Performance Computing Applications},
keywords = {Kokkos library,Performance portability,climate simulations,finite element code,many-core programming},
mendeley-groups = {kEDM},
number = {2},
pages = {332--352},
title = {Toward performance portability of the {Albany} finite element analysis code using the {Kokkos} library},
volume = {33},
year = {2019}
}
@article{Sprague2020,
abstract = {We introduce the open-source ExaWind modeling and simulation environment for wind energy. The primary physics codes of ExaWind are Nalu-Wind and OpenFAST. Nalu-Wind is a wind-focused computational fluid dynamics (CFD) code that is coupled to the whole-turbine simulation code OpenFAST. The ExaWind environment was created under U.S. Department of Energy funding to achieve the highest-fidelity simulations of wind turbines and wind farms to date, with the goal of enabling disruptive changes to turbine and plant design and operation. Innovation will be gleaned through better understanding of the complex flow dynamics in wind farms, including wake evolution and the impact of wakes on downstream turbines and turbulent flow from complex terrain. High-fidelity predictive simulations employ hybrid turbulence models, geometry/boundary-layer-resolving CFD meshes, atmospheric turbulence, nonlinear structural dynamics, and fluid-structure interaction. While there is an emphasis on very high-fidelity simulations (e.g., blade resolved with full fluid-structure coupling), the ExaWind environment supports lower-fidelity modeling capabilities including actuator-line and -disk methods. Important in the development of ExaWind codes is that the codes scale well on today's largest petascale supercomputers and on the next-generation platforms that will enable exascale computing.},
author = {Sprague, M. A. and Ananthan, S. and Vijayakumar, G. and Robinson, M.},
doi = {10.1088/1742-6596/1452/1/012071},
file = {:Users/keichi/Documents/Mendeley Desktop/Sprague{\_}2020{\_}J.{\_}Phys.{\_}{\_}Conf.{\_}Ser.{\_}1452{\_}012071.pdf:pdf},
issn = {17426596},
journal = {Journal of Physics: Conference Series},
mendeley-groups = {kEDM},
number = {1},
title = {{ExaWind}: A multifidelity modeling and simulation environment for wind energy},
volume = {1452},
year = {2020}
}
@inproceedings{Holmen2017,
abstract = {.e University of Utah's Carbon Capture Multidisciplinary Simulation Center (CCMSC) is using the Uintah Computational Framework to predict performance of a 1000 MWe ultra-supercritical clean coal boiler. .e center aims to utilize the Intel Xeon Phi-based DOE systems, .eta and Aurora, through the Aurora Early Science Program by using the Kokkos C++ library to enable node-level performance portability. .is paper describes infrastructure advancements and portability improvements made possible by the integration of Kokkos within Uintah. .is integration marks a step towards consolidating Uintah's MPI+P.reads and MPI+CUDA hybrid parallelism approaches into a single MPI+Kokkos approach. Scalability results are presented that compare serial and data parallel task execution models for a challenging radiative heat transfer calculation, central to the center's predictive boiler simulations. .ese results demonstrate both good strong-scaling characteristics to 256 Knights Landing (KNL) processors on the NSF Stampede system, and show the KNL-based calculation to compete with prior GPU-based results for the same calculation.},
author = {Holmen, John K. and Humphrey, Alan and Sunderland, Daniel and Berzins, Martin},
booktitle = {Practice and Experience in Advanced Research Computing (PEARC'17)},
doi = {10.1145/3093338.3093388},
file = {:Users/keichi/Documents/Mendeley Desktop/3093338.3093388.pdf:pdf},
isbn = {9781450352727},
keywords = {Hybrid Parallelism,Knights Landing,Kokkos,MIC,Many-Core,Parallel,Portability,Radiation Modeling,Reverse Monte-Carlo Ray Tracing,Scalability,Stampede,Uintah,Xeon Phi},
mendeley-groups = {kEDM},
month = jul,
pages = {1--8},
title = {Improving {Uintah's} Scalability Through the Use of Portable {Kokkos-Based} Data Parallel Tasks},
volume = {Part F1287},
year = {2017}
}
@article{Sugihara2012,
abstract = {Identifying causal networks is important for effective policy and management recommendations on climate, epidemiology, financial regulation, and much else. We introduce a method, based on nonlinear state space reconstruction, that can distinguish causality from correlation. It extends to nonseparable weakly connected dynamic systems (cases not covered by the current Granger causality paradigm). The approach is illustrated both by simple models (where, in contrast to the real world, we know the underlying equations/relations and so can check the validity of our method) and by application to real ecological systems, including the controversial sardine-anchovy-temperature problem.},
author = {Sugihara, George and May, Robert and Ye, Hao and Hsieh, Chih Hao and Deyle, Ethan and Fogarty, Michael and Munch, Stephan},
doi = {10.1126/science.1227079},
file = {:Users/keichi/Documents/Mendeley Desktop/Sugihara et al. - 2012 - Detecting causality in complex ecosystems.pdf:pdf},
issn = {10959203},
journal = {Science},
mendeley-groups = {kEDM},
number = {6106},
pages = {496--500},
pmid = {22997134},
title = {{Detecting causality in complex ecosystems}},
volume = {338},
year = {2012}
}
@article{Deyle2011,
abstract = {Takens' theorem (1981) shows how lagged variables of a single time series can be used as proxy variables to reconstruct an attractor for an underlying dynamic process. State space reconstruction (SSR) from single time series has been a powerful approach for the analysis of the complex, non-linear systems that appear ubiquitous in the natural and human world. The main shortcoming of these methods is the phenomenological nature of attractor reconstructions. Moreover, applied studies show that these single time series reconstructions can often be improved ad hoc by including multiple dynamically coupled time series in the reconstructions, to provide a more mechanistic model. Here we provide three analytical proofs that add to the growing literature to generalize Takens' work and that demonstrate how multiple time series can be used in attractor reconstructions. These expanded results (Takens' theorem is a special case) apply to a wide variety of natural systems having parallel time series observations for variables believed to be related to the same dynamic manifold. The potential information leverage provided by multiple embeddings created from different combinations of variables (and their lags) can pave the way for new applied techniques to exploit the time-limited, but parallel observations of natural systems, such as coupled ecological systems, geophysical systems, and financial systems. This paper aims to justify and help open this potential growth area for SSR applications in the natural sciences. {\textcopyright} 2011 Deyle, Sugihara.},
author = {Deyle, Ethan R. and Sugihara, George},
doi = {10.1371/journal.pone.0018295},
file = {:Users/keichi/Documents/Mendeley Desktop/Deyle, Sugihara - 2011 - Generalized theorems for nonlinear state space reconstruction.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
mendeley-groups = {kEDM},
number = {3},
pmid = {21483839},
title = {{Generalized theorems for nonlinear state space reconstruction}},
volume = {6},
year = {2011}
}
@inproceedings{Patwary2016,
abstract = {Computing k-Nearest Neighbors (KNN) is oneof the core kernels used in many machine learning, datamining and scientific computing applications. Although kd-treebased O(log n) algorithms have been proposed for computingKNN, due to its inherent sequentiality, linear algorithms arebeing used in practice. This limits the applicability of suchmethods to millions of data points, with limited scalabilityfor Big Data analytics challenges in the scientific domain. In this paper, we present parallel and highly optimized kdtreebased KNN algorithms (both construction and querying) suitable for distributed architectures. Our algorithm includesnovel approaches for pruning search space and improvingload balancing and partitioning among nodes and threads. Using TB-sized datasets from three science applications: astrophysics, plasma physics, and particle physics, we showthat our implementation can construct kd-tree of 189 billionparticles in 48 seconds on utilizing ∼50,000 cores. We alsodemonstrate computation of KNN of 19 billion queries in12 seconds. We demonstrate almost linear speedup both forshared and distributed memory computers. Our algorithmsoutperforms earlier implementations by more than order ofmagnitude, thereby radically improving the applicability of ourimplementation to state-of-the-art Big Data analytics problems.},
archivePrefix = {arXiv},
arxivId = {1607.08220},
author = {Patwary, Md Mostofa Ali and Satish, Nadathur Rajagopalan and Sundaram, Narayanan and Liu, Jialin and Sadowski, Peter and Racah, Evan and Byna, Suren and Tull, Craig and Bhimji, Wahid and Prabhat and Dubey, Pradeep},
booktitle = {International Parallel and Distributed Processing Symposium (IPDPS)},
doi = {10.1109/IPDPS.2016.57},
eprint = {1607.08220},
file = {:Users/keichi/Documents/Mendeley Desktop/Patwary et al. - 2016 - PANDA Extreme Scale Parallel K-Nearest Neighbor on Distributed Architectures.pdf:pdf},
isbn = {978-1-5090-2140-6},
keywords = {And Load Balancing,Big Data Analytics,Classification,KNN,Kd-tree,Parallel Algorithms},
mendeley-groups = {kEDM},
month = may,
pages = {494--503},
publisher = {IEEE},
title = {{PANDA: Extreme Scale Parallel K-Nearest Neighbor on Distributed Architectures}},
year = {2016}
}
@article{Pan2020,
abstract = {The k-nearest neighbor (KNN) algorithm has been widely used in pattern recognition, regression, outlier detection and other data mining areas. However, it suffers from the large distance computation cost, especially when dealing with big data applications. In this paper, we propose a new fast search (FS) algorithm for exact k-nearest neighbors based on optimal triangle-inequality-based (OTI) check strategy. During the procedure of searching exact k-nearest neighbors for any query, the OTI check strategy can eliminate more redundant distance computations for the instances located in the marginal area of neighboring clusters compared with the original TI check strategy. Considering the large space complexity and extra time complexity of OTI, we also propose an efficient optimal triangle-inequality-based (EOTI) check strategy. The experimental results demonstrate that our proposed two algorithms (OTI and EOTI) achieve the best performance compared with other related KNN fast search algorithms, especially in the case of dealing with high-dimensional datasets.},
author = {Pan, Yiwei and Pan, Zhibin and Wang, Yikun and Wang, Wei},
doi = {10.1016/j.knosys.2019.105088},
file = {:Users/keichi/Documents/Mendeley Desktop/Pan et al. - 2020 - A new fast search algorithm for exact k-nearest neighbors based on optimal triangle-inequality-based check strategy.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Clustering,Exact k-nearest neighbors,Fast search algorithm,Optimal check strategy,Triangle inequality},
mendeley-groups = {kEDM},
pages = {105088},
publisher = {Elsevier B.V.},
title = {{A new fast search algorithm for exact k-nearest neighbors based on optimal triangle-inequality-based check strategy}},
volume = {189},
year = {2020}
}
@inproceedings{Garcia2008,
abstract = {Statistical measures coming from information theory represent interesting bases for image and video processing tasks such as image retrieval and video object tracking. For example, let us mention the entropy and the Kullback-Leibler divergence. Accurate estimation of these measures requires to adapt to the local sample density, especially if the data are high-dimensional. The k nearest neighbor (kNN) framework has been used to define efficient variable-bandwidth kernel-based estimators with such a locally adaptive property. Unfortunately, these estimators are computationally intensive since they rely on searching neighbors among large sets of d-dimensional vectors. This computational burden can be reduced by pre-structuring the data, e.g. using binary trees as proposed by the Approximated Nearest Neighbor (ANN) library. Yet, the recent opening of Graphics Processing Units (GPU) to generalpurpose computation by means of the NVIDIA CUDA API offers the image and video processing community a powerful platform with parallel calculation capabilities. In this paper, we propose a CUDA implementation of the "brute force" kNN search and we compare its performances to several CPU-based implementations including an equivalent brute force algorithm and ANN. We show a speed increase on synthetic and real data by up to one or two orders of magnitude depending on the data, with a quasi-linear behavior with respect to the data size in a given, practical range. {\textcopyright} 2008 IEEE.},
archivePrefix = {arXiv},
arxivId = {0804.1448},
author = {Garcia, Vincent and Debreuve, Eric and Barlaud, Michel},
doi = {10.1109/CVPRW.2008.4563100},
eprint = {0804.1448},
file = {:Users/keichi/Documents/Mendeley Desktop/Garcia, Debreuve, Barlaud - 2008 - Fast k nearest neighbor search using GPU.pdf:pdf},
isbn = {9781424423408},
booktitle = {CVPR Workshop on Computer Vision on GPU},
mendeley-groups = {kEDM},
title = {Fast k nearest neighbor search using {GPU}},
year = {2008}
}
@article{Martineau2017,
abstract = {In this work, we evaluate several emerging parallel programming models: Kokkos, RAJA, OpenACC, and OpenMP 4.0, against the mature CUDA and OpenCL APIs. Each model has been used to port Tealeaf, a miniature proxy application, or mini app, that solves the heat conduction equation and belongs to the Mantevo Project. We find that the best performance is achieved with architecture-specific implementations but that, in many cases, the performance portable models are able to solve the same problems to within a 5{\%} to 30{\%} performance penalty. While the models expose varying levels of complexity to the developer, they all achieve reasonable performance with this application. As such, if this small performance penalty is permissible for a problem domain, we believe that productivity and development complexity can be considered the major differentiators when choosing a modern parallel programming model to develop applications like Tealeaf.},
author = {Martineau, Matthew and McIntosh-Smith, Simon and Gaudin, Wayne},
doi = {10.1002/cpe.4117},
file = {:Users/keichi/Documents/Mendeley Desktop/Martineau, McIntosh-Smith, Gaudin - 2017 - Assessing the performance portability of modern parallel programming models using TeaLeaf.pdf:pdf},
issn = {15320634},
journal = {Concurrency and Computation: Practice and Experience},
keywords = {Kokkos,OpenMP 4.0,RAJA,performance portability,programming models},
mendeley-groups = {kEDM},
number = {15},
pages = {1--15},
title = {Assessing the performance portability of modern parallel programming models using {TeaLeaf}},
volume = {29},
year = {2017}
}
@inproceedings{Deakin2020,
abstract = {With Exascale machines on our immediate horizon, there is a pressing need for applications to be made ready to best exploit these systems. However, there will be multiple paths to Exascale, with each system relying on processor and accelerator technologies from different vendors. As such, applications will be required to be portable between these different architectures, but it is also critical that they are efficient too. These double requirements for portability and efficiency begets the need for performance portability. In this study we survey the performance portability of different programming models, including the open standards OpenMP and SYCL, across the diverse landscape of Exascale and pre-Exascale processors from Intel, AMD, NVIDIA, Fujitsu, Marvell, and Amazon, together encompassing GPUs and CPUs based on both x86 and Arm architectures. We also take a historical view and analyse how performance portability has changed over the last year.},
author = {Deakin, Tom and Poenaru, Andrei and Lin, Tom and McIntosh-Smith, Simon},
booktitle = {P3HPC 2020: International Workshop on Performance, Portability and Productivity in HPC},
mendeley-groups = {kEDM},
title = {Tracking Performance Portability on the Yellow Brick Road to Exascale},
year = {2020}
}
@inproceedings{Schubert2018,
author = {Schubert, Erich and Gertz, Michael},
booktitle = {30th International Conference on Scientific and Statistical Database Management (SSDMB'18)},
doi = {10.1145/3221269.3223036},
file = {:Users/keichi/Documents/Mendeley Desktop/Schubert, Gertz - 2018 - Numerically stable parallel computation of (co-)variance.pdf:pdf},
isbn = {9781450365055},
mendeley-groups = {kEDM},
month = jul,
pages = {1--12},
title = {{Numerically stable parallel computation of (co-)variance}},
year = {2018}
}
@inproceedings{Treibig2010,
author = {Treibig, Jan and Hager, Georg and Wellein, Gerhard},
booktitle = {39th International Conference on Parallel Processing Workshops},
doi = {10.1109/ICPPW.2010.38},
file = {:Users/keichi/Downloads/1004.4431.pdf:pdf},
isbn = {978-1-4244-7918-4},
mendeley-groups = {kEDM},
month = sep,
pages = {207--216},
title = {{LIKWID}: A Lightweight Performance-Oriented Tool Suite for x86 Multicore Environments},
year = {2010}
}
@article{Yang2020b,
abstract = {The Roofline performance model provides an intuitive and insightful approach to identifying performance bottlenecks and guiding performance optimization. In preparation for the next-generation supercomputer Perlmutter at NERSC, this paper presents a methodology to construct a hierarchical Roofline on NVIDIA GPUs and extends it to support reduced precision and Tensor Cores. The hierarchical Roofline incorporates L1, L2, device memory, and system memory bandwidths into one single figure, and it offers more profound insights into performance analysis than the traditional DRAM-only Roofline. We use our Roofline methodology to analyze three proxy applications: GPP from BerkeleyGW, HPGMG from AMReX, and conv2d from TensorFlow. In doing so, we demonstrate the ability of our methodology to readily understand various aspects of performance and performance bottlenecks on NVIDIA GPUs and motivate code optimizations.},
author = {Yang, Charlene and Kurth, Thorsten and Williams, Samuel},
doi = {10.1002/cpe.5547},
file = {:Users/keichi/Documents/Mendeley Desktop/Yang, Kurth, Williams - 2020 - Hierarchical Roofline analysis for GPUs Accelerating performance optimization for the NERSC-9 Perlmutter.pdf:pdf},
issn = {15320634},
journal = {Concurrency and Computation: Practice and Experience},
keywords = {Cray,NVIDIA GPU,Roofline,code optimization,performance analysis,tensor core},
mendeley-groups = {kEDM},
month = oct,
number = {20},
title = {Hierarchical Roofline analysis for {GPUs}: Accelerating performance optimization for the {NERSC-9} {Perlmutter} system},
volume = {32},
year = {2020}
}
@article{Chang2017,
abstract = {Natural systems are often complex and dynamic (i.e. nonlinear), making them difficult to understand using linear statistical approaches. Linear approaches are fundamentally based on correlation. Thus, they are ill-posed for dynamical systems, where correlation can occur without causation, and causation may also occur in the absence of correlation. “Mirage correlation” (i.e., the sign and magnitude of the correlation change with time) is a hallmark of nonlinear systems that results from state dependency. State dependency means that the relationships among interacting variables change with different states of the system. In recent decades, nonlinear methods that acknowledge state dependence have been developed. These nonlinear statistical methods are rooted in state space reconstruction, i.e. lagged coordinate embedding of time series data. These methods do not assume any set of equations governing the system but recover the dynamics from time series data, thus called empirical dynamic modeling (EDM). EDM bears a variety of utilities to investigating dynamical systems. Here, we provide a step-by-step tutorial for EDM applications with rEDM, a free software package written in the R language. Using model examples, we aim to guide users through several basic applications of EDM, including (1) determining the complexity (dimensionality) of a system, (2) distinguishing nonlinear dynamical systems from linear stochastic systems, and quantifying the nonlinearity (i.e. state dependence), (3) determining causal variables, (4) forecasting, (5) tracking the strength and sign of interaction, and (6) exploring the scenario of external perturbation. These methods and applications can be used to provide a mechanistic understanding of dynamical systems.},
author = {Chang, Chun Wei and Ushio, Masayuki and Hao Hsieh, Chih},
doi = {10.1007/s11284-017-1469-9},
file = {:Users/keichi/Documents/Mendeley Desktop/Chang, Ushio, Hsieh - 2017 - Empirical dynamic modeling for beginners.pdf:pdf},
issn = {14401703},
journal = {Ecological Research},
keywords = {Embedding,Forecast,Interaction,State dependence,State space reconstruction},
mendeley-groups = {kEDM},
month = nov,
number = {6},
pages = {785--796},
publisher = {Springer Tokyo},
title = {Empirical dynamic modeling for beginners},
volume = {32},
year = {2017}
}
@article{Edwards2014,
author = {{Carter Edwards}, H. and Trott, Christian R. and Sunderland, Daniel},
doi = {10.1016/j.jpdc.2014.07.003},
file = {:Users/keichi/Downloads/1-s2.0-S0743731514001257-main.pdf:pdf},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
mendeley-groups = {kEDM},
month = dec,
number = {12},
title = {{Kokkos: Enabling manycore performance portability through polymorphic memory access patterns}},
volume = {74},
year = {2014}
}
@inproceedings{Matthes2017,
author = {Matthes, Alexander and Widera, Ren{\'{e}} and Zenker, Erik and Worpitz, Benjamin and Huebl, Axel and Bussmann, Michael},
booktitle = {2nd International Workshop on Performance Portable Programming Models for Accelerators (P{\^{}}3MA)},
doi = {10.1007/978-3-319-67630-2_36},
file = {:Users/keichi/Documents/Mendeley Desktop/1706.10086.pdf:pdf},
mendeley-groups = {kEDM},
pages = {496--514},
title = {{Tuning and Optimization for a Variety of Many-Core Architectures Without Changing a Single Line of Implementation Code Using the Alpaka Library}},
year = {2017}
}
@inproceedings{Pu2019,
author = {Pu, Bo and Duan, Lujie and Osgood, Nathaniel D.},
booktitle = {International Conference on Social Computing, Behavioral-Cultural Modeling, {\&} Prediction and Behavior Representation in Modeling and Simulation (SBP-BRiMS 2019)},
doi = {10.1007/978-3-030-21741-9_14},
file = {:Users/keichi/Documents/Mendeley Desktop/1905.00565.pdf:pdf},
mendeley-groups = {kEDM},
pages = {133--142},
title = {Parallelizing Convergent Cross Mapping Using {Apache} {Spark}},
year = {2019}
}
@inproceedings{mpedm,
author = {Watanakeesuntorn, Wassapon and Takahashi, Keichi and Ichikawa, Kohei and Park, Joseph and Sugihara, George and Takano, Ryousei and Haga, Jason and Pao, Gerald M.},
booktitle = {26th International Conference on Parallel and Distributed Systems (ICPADS 2020)},
file = {:Users/keichi/Documents/Mendeley Desktop/Watanakeesuntorn et al. - 2020 - Massively Parallel Causal Inference of Whole Brain Dynamics at Single Neuron Resolution.pdf:pdf},
mendeley-groups = {kEDM},
month = dec,
title = {{Massively Parallel Causal Inference of Whole Brain Dynamics at Single Neuron Resolution}},
year = {2020}
}
@article{Natsukawa2020,
abstract = {Fig. 1. Schematic of visual analytics to study nonlinear interactions with empirical dynamic modeling (EDM). A dynamic graph of changing interaction coefficients is first constructed using the (A) measured time series data and (B) interaction coefficients extracted via EDM techniques. (C) Our proposed visual analytics system enables the detection and interpretation of system states using dimension reduction and brush-link visualization techniques. (D) Using the process of annotation and summarization, the state transition graph can be obtained for interpretation. Abstract-An important approach for scientific inquiry across many disciplines involves using observational time series data to understand the relationships between key variables to gain mechanistic insights into the underlying rules that govern the given system. In real systems, such as those found in ecology, the relationships between time series variables are generally not static; instead, these relationships are dynamical and change in a nonlinear or state-dependent manner. To further understand such systems, we investigate integrating methods that appropriately characterize these dynamics (i.e., methods that measure interactions as they change with time-varying system states) with visualization techniques that can help analyze the behavior of the system. Here, we focus on empirical dynamic modeling (EDM) as a state-of-the-art method that specifically identifies causal variables and measures changing state-dependent relationships between time series variables. Instead of using approaches centered on parametric equations, EDM is an equation-free approach that studies systems based on their dynamic attractors. We propose a visual analytics system to support the identification and mechanistic interpretation of system states using an EDM-constructed dynamic graph. This work, as detailed in four analysis tasks and demonstrated with a GUI, provides a novel synthesis of EDM and visualization techniques such as brush-link visualization and visual summarization to interpret dynamic graphs representing ecosystem dynamics. We applied our proposed system to ecological simulation data and real data from a marine mesocosm study as two key use cases. Our case studies show that our visual analytics tools support the identification and interpretation of the system state by the user, and enable us to discover both confirmatory and new findings in ecosystem dynamics. Overall, we demonstrated that our system can facilitate an understanding of how systems function beyond the intuitive analysis of high-dimensional information based on specific domain knowledge.},
author = {Natsukawa, Hiroaki and Deyle, Ethan R. and Pao, Gerald M. and Koyamada, Koji and Sugihara, George},
doi = {10.1109/tvcg.2020.3028956},
file = {:Users/keichi/Documents/Mendeley Desktop/09216532.pdf:pdf},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
mendeley-groups = {kEDM},
number = {c},
pages = {1--1},
title = {{A Visual Analytics Approach for Ecosystem Dynamics based on Empirical Dynamic Modeling}},
volume = {2626},
year = {2020}
}
@inproceedings{Natsukawa2017,
author = {Natsukawa, Hiroaki and Koyamada, Koji},
booktitle = {SIGGRAPH Asia 2017 Symposium on Visualization (SA'17)},
doi = {10.1145/3139295.3139303},
file = {:Users/keichi/Documents/Mendeley Desktop/Natsukawa, Koyamada - 2017 - Visual analytics of brain effective connectivity using convergent cross mapping.pdf:pdf},
isbn = {9781450354110},
mendeley-groups = {kEDM},
month = nov,
title = {Visual analytics of brain effective connectivity using convergent cross mapping},
year = {2017}
}
@article{Arefin2012,
abstract = {Background: The analysis of biological networks has become a major challenge due to the recent development of high-throughput techniques that are rapidly producing very large data sets. The exploding volumes of biological data are craving for extreme computational power and special computing facilities (i.e. super-computers). An inexpensive solution, such as General Purpose computation based on Graphics Processing Units (GPGPU), can be adapted to tackle this challenge, but the limitation of the device internal memory can pose a new problem of scalability. An efficient data and computational parallelism with partitioning is required to provide a fast and scalable solution to this problem. Results: We propose an efficient parallel formulation of the k-Nearest Neighbour (kNN) search problem, which is a popular method for classifying objects in several fields of research, such as pattern recognition, machine learning and bioinformatics. Being very simple and straightforward, the performance of the kNN search degrades dramatically for large data sets, since the task is computationally intensive. The proposed approach is not only fast but also scalable to large-scale instances. Based on our approach, we implemented a software tool GPU-FS-kNN (GPU-based Fast and Scalable k-Nearest Neighbour) for CUDA enabled GPUs. The basic approach is simple and adaptable to other available GPU architectures. We observed speed-ups of 50-60 times compared with CPU implementation on a well-known breast microarray study and its associated data sets. Conclusion: Our GPU-based Fast and Scalable k-Nearest Neighbour search technique (GPU-FS-kNN) provides a significant performance improvement for nearest neighbour computation in large-scale networks. Source code and the software tool is available under GNU Public License (GPL) at https://sourceforge.net/p/gpufsknn/. {\textcopyright} 2012 Arefin et al.},
author = {Arefin, Ahmed Shamsul and Riveros, Carlos and Berretta, Regina and Moscato, Pablo},
doi = {10.1371/journal.pone.0044000},
file = {:Users/keichi/Documents/Mendeley Desktop/Arefin et al. - 2012 - GPU-FS-kNN A Software Tool for Fast and Scalable kNN Computation Using GPUs.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
mendeley-groups = {kEDM},
number = {8},
pmid = {22937144},
title = {{GPU-FS-kNN: A Software Tool for Fast and Scalable kNN Computation Using GPUs}},
volume = {7},
year = {2012}
}
@inproceedings{Shanbhag2018,
abstract = {A common operation in many data analytics workloads is to find the top-k items, i.e., the largest or smallest operations according to some sort order (implemented via LIMIT or ORDER BY expressions in SQL). A naive implementation of top-k is to sort all of the items and then return the first k, but this does much more work than needed. Although efficient implementations for top-k have been explored on traditional multi-core processors, there has been no prior systematic study of top-k implementations on GPUs, despite open requests for such implementations in GPU-based frameworks like TensorFlow1 and ArrayFire2. In this work, we present several top-k algorithms for GPUs, including a new algorithm based on bitonic sort called bitonic top-k. The bitonic top-k algorithm is up to a factor of 15x faster than sort and 4x faster than a variety of other possible implementations for values of k up to 256. We also develop a cost model to predict the performance of several of our algorithms, and show that it accurately predicts actual performance on modern GPUs.},
author = {Shanbhag, Anil and Pirk, Holger and Madden, Samuel},
booktitle = {International Conference on Management of Data (SIGMOD '18)},
doi = {10.1145/3183713.3183735},
file = {:Users/keichi/Documents/Mendeley Desktop/Shanbhag, Pirk, Madden - 2018 - Efficient Top-K Query Processing on Massively Parallel Hardware.pdf:pdf},
isbn = {9781450347037},
issn = {07308078},
keywords = {Bitonic Top-K,Top-K algorithms for GPU},
mendeley-groups = {kEDM},
pages = {1557--1570},
title = {Efficient Top-K Query Processing on Massively Parallel Hardware},
year = {2018}
}
@article{Johnson2019,
abstract = {Similarity search finds application in specialized database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data-parallel tasks, prior approaches are bot-tlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy. We propose a design for k-selection that operates at up to 55{\%} of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5× faster than prior GPU state of the art. We apply it in different similarity search scenarios, by proposing optimized design for brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation enables the construction of a high accuracy k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach 1 for the sake of comparison and reproducibility.},
annote = {近似近傍探索ライブラリ Faiss の元論文},
archivePrefix = {arXiv},
arxivId = {1702.08734v1},
author = {Johnson, Jeff and Douze, Matthijs and Jegou, Herve},
doi = {10.1109/TBDATA.2019.2921572},
eprint = {1702.08734v1},
file = {:Users/keichi/Documents/Mendeley Desktop/Johnson, Douze, Jegou - 2019 - Billion-scale similarity search with GPUs.pdf:pdf},
issn = {2332-7790},
journal = {IEEE Transactions on Big Data},
mendeley-groups = {kEDM},
pages = {1--1},
title = {Billion-scale similarity search with {GPUs}},
year = {2019}
}
@article{Garcia2010,
abstract = {The k-nearest neighbor (kNN) search problem is widely used in domains and applications such as classification, statistics, and biology. In this paper, we propose two fast GPU-based implementations of the brute-force kNN search algorithm using the CUDA and CUBLAS APIs. We show that our CUDA and CUBLAS implementations are up to, respectively, 64X and 189X faster on synthetic data than the highly optimized ANN C++ library, and up to, respectively, 25X and 62X faster on high-dimensional SIFT matching. {\textcopyright} 2010 IEEE.},
author = {Garcia, Vincent and Debreuve, Eŕic and Nielsen, Frank and Barlaud, Michel},
doi = {10.1109/ICIP.2010.5654017},
file = {:Users/keichi/Documents/Mendeley Desktop/Garcia et al. - 2010 - K-nearest neighbor search Fast GPU-based implementations and application to high-dimensional feature matching.pdf:pdf},
isbn = {9781424479948},
issn = {15224880},
journal = {International Conference on Image Processing (ICIP)},
keywords = {CUDA/CUBLAS,GPU,SIFT,k-nearest neighbors},
mendeley-groups = {kEDM},
month = sep,
pages = {3757--3760},
title = {{k}-nearest neighbor search: Fast {GPU}-based implementations and application to high-dimensional feature matching},
year = {2010}
}
@inproceedings{Deakin2019,
abstract = {Previous studies into performance portability have typically analysed a single application (and its various imple- mentations) in isolation. In this study we explore the wider landscape of performance portability by considering a number of applications from across the space of dwarfs, written in multiple parallel programming models, and across a diverse set of architectures. We apply rigorous performance portability metrics, as defined by Pennycook et al [1]. We believe this is the broadest and most rigorous performance portability study to date, representing a far reaching exploration of the state of performance portability that is achievable today. We will present a summary of the performance portability of each application and programming model across our diverge range of twelve computer architectures, including six different server CPUs from five different vendors, five different GPUs from two different vendors, and one vector architecture. We will conclude with an analysis of the performance portability of key programming models in general, across different application spaces as well across differing architectures, allowing us to comment on more general performance portability principles.},
author = {Deakin, Tom and Mcintosh-Smith, Simon and Price, James and Poenaru, Andrei and Atkinson, Patrick and Popa, Codrin and Salmon, Justin},
booktitle = {International Workshop on Performance, Portability and Productivity in HPC (P3HPC 2019)},
doi = {10.1109/P3HPC49587.2019.00006},
file = {:Users/keichi/Documents/Mendeley Desktop/Deakin et al. - 2019 - Performance Portability across Diverse Computer Architectures.pdf:pdf},
isbn = {9781728160030},
keywords = {mini-app,performance portability,productivity,programming models},
mendeley-groups = {kEDM},
month = nov,
pages = {1--13},
title = {Performance Portability across Diverse Computer Architectures},
year = {2019}
}
@unpublished{Yang2020a,
abstract = {This paper surveys a range of methods to collect necessary performance data on Intel CPUs and NVIDIA GPUs for hierarchical Roofline analysis. As of mid-2020, two vendor performance tools, Intel Advisor and NVIDIA Nsight Compute, have integrated Roofline analysis into their supported feature set. This paper fills the gap for when these tools are not available, or when users would like a more customized workflow for certain analysis. Specifically, we will discuss how to use Intel Advisor, RRZE LIKWID, Intel SDE and Intel Amplifier on Intel architectures, and nvprof, Nsight Compute metrics, and Nsight Compute section files on NVIDIA architectures. These tools will be used to collect information for as many memory/cache levels in the memory hierarchy as possible in order to provide insights into application's data reuse and cache locality characteristics.},
archivePrefix = {arXiv},
arxivId = {2009.02449},
author = {Yang, Charlene},
eprint = {2009.02449},
file = {:Users/keichi/Documents/Mendeley Desktop/Yang - 2020 - Hierarchical Roofline Analysis How to Collect Data using Performance Tools on Intel CPUs and NVIDIA GPUs.pdf:pdf},
mendeley-groups = {kEDM},
month = sep,
title = {Hierarchical Roofline Analysis: How to Collect Data using Performance Tools on {Intel} {CPUs} and {NVIDIA} {GPUs}},
url = {http://arxiv.org/abs/2009.02449},
year = {2020}
}
@article{Williams2008,
abstract = {We propose an easy-to-understand, visual performance model that offers insights to programmers and architects on improving parallel software and hardware for floating point computations.},
author = {Williams, Samuel and Waterman, Andrew and Patterson, David},
doi = {10.1145/1498765.1498785},
file = {:Users/keichi/Documents/Mendeley Desktop/Williams, Waterman, Patterson - 2008 - Roofline An Insightful Visual Performance Model for Floating-Point Programs and Multicore Archite.pdf:pdf},
journal = {Communications of the ACM},
mendeley-groups = {kEDM},
number = {4},
pages = {65--76},
title = {Roofline: An Insightful Visual Performance Model for Floating-Point Programs and Multicore Architectures},
volume = {52},
year = {2008}
}
@inproceedings{Ma2017,
author = {Ma, Jiayi and Yang, Ming and Han, Xueshan and Li, Zhi},
booktitle = {2017 IEEE Industry Applications Society Annual Meeting},
doi = {10.1109/IAS.2017.8101715},
file = {:Users/keichi/Documents/Mendeley Desktop/Ma et al. - 2017 - Ultra-short-term wind generation forecast based on multivariate empirical dynamic modeling.pdf:pdf},
isbn = {978-1-5090-4894-6},
mendeley-groups = {kEDM},
month = oct,
title = {{Ultra-short-term wind generation forecast based on multivariate empirical dynamic modeling}},
year = {2017}
}
@inproceedings{Malcolm2012,
abstract = {ArrayFire is a GPU matrix library for the rapid development of general purpose GPU (GPGPU) computing applications within C, C++, Fortran, and Python. ArrayFire contains a simple API and provides full GPU compute capability on CUDA and OpenCL capable devices. ArrayFire provides thousands of GPU-tuned functions including linear algebra, convolutions, reductions, and FFTs as well as signal, image, statistics, and graphics libraries. We will further describe how ArrayFire enables development of GPU computing applications and highlight some of its key functionality using examples of how it works in real code.},
author = {Malcolm, James and Yalamanchili, Pavan and McClanahan, Chris and Venugopalakrishnan, Vishwanath and Patel, Krunal and Melonakos, John},
booktitle = {Modeling and Simulation for Defense Systems and Applications VII},
doi = {10.1117/12.921122},
file = {:Users/keichi/Documents/Mendeley Desktop/Malcolm et al. - 2012 - ArrayFire a GPU acceleration platform.pdf:pdf},
isbn = {9780819490810},
issn = {1996756X},
mendeley-groups = {kEDM},
month = may,
pages = {84030A},
title = {{ArrayFire: a GPU acceleration platform}},
volume = {8403},
year = {2012}
}
@article{Ye2015,
  title={Equation-free mechanistic ecosystem forecasting using empirical dynamic modeling},
  author={Ye, Hao and Beamish, Richard J and Glaser, Sarah M and Grant, Sue CH and Hsieh, Chih-hao and Richards, Laura J and Schnute, Jon T and Sugihara, George},
  journal={Proceedings of the National Academy of Sciences},
  volume={112},
  number={13},
  pages={E1569--E1576},
  year={2015},
}
@incollection{Takens1981,
  title={Detecting strange attractors in turbulence},
  author={Takens, Floris},
  booktitle={Dynamical systems and turbulence, Warwick 1980},
  pages={366--381},
  year={1981},
  publisher={Springer}
}