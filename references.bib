@inproceedings{Ma2017,
   author = {Jiayi Ma and Ming Yang and Xueshan Han and Zhi Li},
   doi = {10.1109/IAS.2017.8101715},
   isbn = {978-1-5090-4894-6},
   journal = {2017 IEEE Industry Applications Society Annual Meeting},
   month = {10},
   publisher = {IEEE},
   title = {Ultra-short-term wind generation forecast based on multivariate empirical dynamic modeling},
   year = {2017},
}
@inproceedings{Natsukawa2017,
   author = {Hiroaki Natsukawa and Koji Koyamada},
   city = {New York, NY, USA},
   doi = {10.1145/3139295.3139303},
   isbn = {9781450354110},
   journal = {SIGGRAPH Asia 2017 Symposium on Visualization},
   month = {11},
   publisher = {ACM},
   title = {Visual analytics of brain effective connectivity using convergent cross mapping},
   year = {2017},
}
@inproceedings{Deakin2019,
   abstract = {Previous studies into performance portability have typically analysed a single application (and its various imple- mentations) in isolation. In this study we explore the wider landscape of performance portability by considering a number of applications from across the space of dwarfs, written in multiple parallel programming models, and across a diverse set of architectures. We apply rigorous performance portability metrics, as defined by Pennycook et al [1]. We believe this is the broadest and most rigorous performance portability study to date, representing a far reaching exploration of the state of performance portability that is achievable today. We will present a summary of the performance portability of each application and programming model across our diverge range of twelve computer architectures, including six different server CPUs from five different vendors, five different GPUs from two different vendors, and one vector architecture. We will conclude with an analysis of the performance portability of key programming models in general, across different application spaces as well across differing architectures, allowing us to comment on more general performance portability principles.},
   author = {Tom Deakin and Simon Mcintosh-Smith and James Price and Andrei Poenaru and Patrick Atkinson and Codrin Popa and Justin Salmon},
   doi = {10.1109/P3HPC49587.2019.00006},
   isbn = {9781728160030},
   journal = {Proceedings of P3HPC 2019: International Workshop on Performance, Portability and Productivity in HPC - Held in conjunction with SC 2019: The International Conference for High Performance Computing, Networking, Storage and Analysis},
   keywords = {mini-app,performance portability,productivity,programming models},
   month = {11},
   pages = {1-13},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Performance Portability across Diverse Computer Architectures},
   year = {2019},
}
@article{Yang2020,
   abstract = {The Roofline performance model provides an intuitive and insightful approach to identifying performance bottlenecks and guiding performance optimization. In preparation for the next-generation supercomputer Perlmutter at NERSC, this paper presents a methodology to construct a hierarchical Roofline on NVIDIA GPUs and extends it to support reduced precision and Tensor Cores. The hierarchical Roofline incorporates L1, L2, device memory, and system memory bandwidths into one single figure, and it offers more profound insights into performance analysis than the traditional DRAM-only Roofline. We use our Roofline methodology to analyze three proxy applications: GPP from BerkeleyGW, HPGMG from AMReX, and conv2d from TensorFlow. In doing so, we demonstrate the ability of our methodology to readily understand various aspects of performance and performance bottlenecks on NVIDIA GPUs and motivate code optimizations.},
   author = {Charlene Yang and Thorsten Kurth and Samuel Williams},
   doi = {10.1002/cpe.5547},
   issn = {15320634},
   issue = {20},
   journal = {Concurrency Computation },
   keywords = {Cray,NVIDIA GPU,Roofline,code optimization,performance analysis,tensor core},
   month = {10},
   publisher = {John Wiley and Sons Ltd},
   title = {Hierarchical Roofline analysis for GPUs: Accelerating performance optimization for the NERSC-9 Perlmutter system},
   volume = {32},
   year = {2020},
}
@inproceedings{Malcolm2012,
   abstract = {ArrayFire is a GPU matrix library for the rapid development of general purpose GPU (GPGPU) computing applications within C, C++, Fortran, and Python. ArrayFire contains a simple API and provides full GPU compute capability on CUDA and OpenCL capable devices. ArrayFire provides thousands of GPU-tuned functions including linear algebra, convolutions, reductions, and FFTs as well as signal, image, statistics, and graphics libraries. We will further describe how ArrayFire enables development of GPU computing applications and highlight some of its key functionality using examples of how it works in real code.},
   author = {James Malcolm and Pavan Yalamanchili and Chris McClanahan and Vishwanath Venugopalakrishnan and Krunal Patel and John Melonakos},
   doi = {10.1117/12.921122},
   isbn = {9780819490810},
   issn = {1996756X},
   journal = {Modeling and Simulation for Defense Systems and Applications VII},
   month = {5},
   pages = {84030A},
   publisher = {SPIE},
   title = {ArrayFire: a GPU acceleration platform},
   volume = {8403},
   year = {2012},
}
@article{Chang2017,
   abstract = {Natural systems are often complex and dynamic (i.e. nonlinear), making them difficult to understand using linear statistical approaches. Linear approaches are fundamentally based on correlation. Thus, they are ill-posed for dynamical systems, where correlation can occur without causation, and causation may also occur in the absence of correlation. “Mirage correlation” (i.e., the sign and magnitude of the correlation change with time) is a hallmark of nonlinear systems that results from state dependency. State dependency means that the relationships among interacting variables change with different states of the system. In recent decades, nonlinear methods that acknowledge state dependence have been developed. These nonlinear statistical methods are rooted in state space reconstruction, i.e. lagged coordinate embedding of time series data. These methods do not assume any set of equations governing the system but recover the dynamics from time series data, thus called empirical dynamic modeling (EDM). EDM bears a variety of utilities to investigating dynamical systems. Here, we provide a step-by-step tutorial for EDM applications with rEDM, a free software package written in the R language. Using model examples, we aim to guide users through several basic applications of EDM, including (1) determining the complexity (dimensionality) of a system, (2) distinguishing nonlinear dynamical systems from linear stochastic systems, and quantifying the nonlinearity (i.e. state dependence), (3) determining causal variables, (4) forecasting, (5) tracking the strength and sign of interaction, and (6) exploring the scenario of external perturbation. These methods and applications can be used to provide a mechanistic understanding of dynamical systems.},
   author = {Chun Wei Chang and Masayuki Ushio and Chih hao Hsieh},
   doi = {10.1007/s11284-017-1469-9},
   issn = {14401703},
   issue = {6},
   journal = {Ecological Research},
   keywords = {Embedding,Forecast,Interaction,State dependence,State space reconstruction},
   month = {11},
   pages = {785-796},
   publisher = {Springer Tokyo},
   title = {Empirical dynamic modeling for beginners},
   volume = {32},
   year = {2017},
}
@article{Williams2008,
   abstract = {We propose an easy-to-understand, visual performance model that offers insights to programmers and architects on improving parallel software and hardware for floating point computations.},
   author = {Samuel Williams and Andrew Waterman and David Patterson},
   doi = {10.1145/1498765.1498785},
   issue = {4},
   journal = {Communications of the ACM},
   pages = {65-76},
   title = {Roofline: An Insightful Visual Performance Model for Floating-Point Programs and Multicore Architectures*},
   volume = {52},
   year = {2008},
}
@article{,
   author = {H. Carter Edwards and Christian R. Trott and Daniel Sunderland},
   doi = {10.1016/j.jpdc.2014.07.003},
   issn = {07437315},
   issue = {12},
   journal = {Journal of Parallel and Distributed Computing},
   month = {12},
   title = {Kokkos: Enabling manycore performance portability through polymorphic memory access patterns},
   volume = {74},
   year = {2014},
}
@inproceedings{Watanakeesuntorn2020,
   author = {Wassapon Watanakeesuntorn and Keichi Takahashi and Kohei Ichikawa and Joseph Park and George Sugihara and Ryousei Takano and Jason Haga and Gerald M. Pao},
   journal = {26th International Conference on Parallel and Distributed Systems (ICPADS 2020)},
   month = {12},
   title = {Massively Parallel Causal Inference of Whole Brain Dynamics at Single Neuron Resolution},
   year = {2020},
}
