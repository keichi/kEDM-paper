\documentclass[conference]{IEEEtran}

\usepackage{amsmath}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{booktabs}
\usepackage[hang]{caption}
\usepackage{cite}
% \usepackage{flushend}
\usepackage[bottom,hang]{footmisc}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{icomma}
\usepackage{listings}
\usepackage{newtxtext}
\usepackage{newtxmath}
\usepackage{url}
\usepackage{xcolor}
\RequirePackage[l2tabu, orthodox]{nag}

% Allow PDF 1.7 documents to be included with \includegraphics
\pdfminorversion=7

% Suppress "multiple pdfs with page group included in a single page" warning
\pdfsuppresswarningpagegroup=1

% Define colors for code highlighting
\definecolor{color-bg}{HTML}{F6F8FA}
\definecolor{color-keyword}{HTML}{D73A49}
\definecolor{color-ident}{HTML}{005CC5}
\definecolor{color-string}{HTML}{032F62}
\definecolor{color-comment}{HTML}{6A737D}

% Listing
\lstset{%
  language={C++},
  basicstyle={\small\ttfamily},%
  backgroundcolor=\color{color-bg},%
  identifierstyle={\small\ttfamily},%
  commentstyle={\small\itshape\color{color-comment}},%
  keywordstyle={\small\bfseries\color{color-ident}},%
  % ndkeywordstyle={\small\ttfamily},%
  stringstyle={\small\ttfamily\color{color-string}},%
  frame={trbl},%
  frameround={tttt},%
  breaklines=true,%
  columns=[l]{fullflexible},%
  numbers=left,%
  numberstyle={\scriptsize},%
  stepnumber=1,%
  numbersep=1em,%
  % lineskip=-0.5ex,%
  mathescape,%
  xleftmargin=2em,%
  framexleftmargin=1.5em,%
}

\begin{document}

% algorithme2
\SetKwFor{PFor}{parallel for}{do}{end}

\title{kEDM: A Performance-portable Implementation of Empirical Dynamical Modeling}

\author{%
    \IEEEauthorblockN{%
        Keichi Takahashi,\\Wassapon Watanakeesuntorn,\\Kohei Ichikawa
    } \\
    \IEEEauthorblockA{%
        Nara Institute of Science and Technology\\
        Nara, Japan\\
        \{keichi, wassapon.watanakeesuntorn.wq0,\\ ichikawa\}@is.naist.jp
    }
    \and
    \IEEEauthorblockN{%
        George Sugihara
    } \\
    \IEEEauthorblockA{%
        University of California San Diego\\
        California, USA\\
        gsugihara@ucsd.edu
    }
    \and
    \IEEEauthorblockN{%
        Gerald M. Pao
    } \\
    \IEEEauthorblockA{%
        Salk Institute for Biological Studies\\
        California, USA\\
        pao@salk.edu
    }
}

\maketitle

\begin{abstract}
Recent rapid scale out of high performance computing systems has
rapidly and continuously increased the scale and complexity of the
interconnects. As a result, current static and over-provisioned
interconnects are becoming cost-ineffective. Against this background, we have
been working on the integration of network programmability into
the interconnect control, based on the idea that dynamically controlling
the packet flow in the interconnect according to the communication pattern
of applications can increase the utilization of interconnects and improve
application performance. Interconnect simulators come in handy especially
when investigating the performance characteristics of interconnects with
different topologies and parameters. However, little effort has been put
towards the simulation of packet flow in dynamically controlled interconnects,
while simulators for static interconnects have been extensively researched
and developed. To facilitate analysis on the performance
characteristics of dynamic interconnects, we have developed PFAnalyzer.
PFAnalyzer is a toolset composed of PFSim, an interconnect simulator
specialized for dynamic interconnects, and PFProf, a profiler.
PFSim allows interconnect researchers and designers to investigate
congestion in the interconnect for an arbitrary cluster configuration and
a set of communication patterns collected by PFProf. PFAnalyzer is used
to demonstrate how dynamically controlling the interconnects can reduce
congestion and potentially improve the performance of applications.
\end{abstract}

\begin{IEEEkeywords}
    Empirical Dynamical Modeling, Performance Portability, High-Performance
    Computing
\end{IEEEkeywords}

\section{Introduction}

Empirical Dynamical Modeling (EDM)~\cite{Chang2017} is a state-of-the-art
non-linear time series analysis framework for various tasks such as assessing
the non-linearity of a system, making short-term forecasts, and identifying
the existence and strength of causal relationships between variables. Despite
its wide applicability, EDM is computationally expensive and thus the scale of
the datasets that can be applied to was limited. To overcome this obstacle,
researchers have attempted to accelerate EDM  from both algorithmic and
implementational aspects~\cite{Pu2019,Ma2017}.

Our approach for accelerating EDM is to take advantage of modern HPC systems
equipped with multi-core CPUs and GPUs. We have been developing mpEDM, a
parallel distributed implementation of EDM optimized for GPU-centric HPC
systems. In our previous work~\cite{mpedm}, we have executed mpEDM on
the AI Bridging Cloud Infrastructure (ABCI)\footnote{\url{https://abci.ai/}}
to obtain an all-to-all causal relationship map of all $10^5$ neurons in a
larval zebrafish brain. To this date, this is the first causal analysis of a
whole vertebrate brain at single neuron resolution.

Although mpEDM has successfully demonstrated the benefit of using HPC systems
to achieve EDM computation at an unprecedented scale, there are still
challenges remained. The primary challenge is \textit{performance portability}.
Recent HPC landscape has seen significant increase in diversity of processors
and accelerators. This is reflected in the design of upcoming exascale HPC
systems: the Frontier system at the Oak Ridge National Laboratory will use AMD
EPYC CPUs and Radeon GPUs while the Aurora system at the Argonne National
Laboratory will employ Intel Xeon CPUs and Xe GPUs. The Fugaku system at RIKEN
uses Fujitsu A64FX Arm CPUs.

Currently, mpEDM has two backends for x86-64 CPUs and NVIDIA GPUs
independently developed and maintained. This design becomes a burden when
targeting a new hardware architecture, because a new backend needs be added,
which requires development effort and raises maintenance cost. On the other
hand, various performance portability frameworks~\cite{Deakin2019, Deakin2020}
have been proposed to develop performance-portable applications from
singe-source code base.

In this paper, we use the Kokkos~\cite{Edwards2014} framework and develop
\textit{kEDM}, a novel performance-portable implementation of EDM\@.
The new implementation is single-source design which facilitates the future
development and porting to new hardware. This also makes it possible to run
individual kernels on CPUs or accelerators on hybrid systems. Furthermore, we
identify and take advantage of optimization opportunities in mpEDM and achieve
3--4$\times$ higher performance on NVIDIA V100 GPUs.

\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{figs/xmap_overview}
    \caption{Overview of Convergent Cross Mapping}%
    \label{fig:edm}
\end{figure*}

The rest of this paper is organized as follows. Section~\ref{sec:background}
first introduces EDM briefly and then discusses the challenges in mpEDM\@.
Section~\ref{sec:proposal} presents kEDM, a novel performance-portable
implementation of EDM based on the Kokkos performance portability framework.
Section~\ref{sec:evaluation} compares kEDM and mpEDM using both synthetic and
real-world datasets and assesses the efficiency of kEDM\@.
Section~\ref{sec:conclusion} concludes this paper and discusses future work.

\section{Background}\label{sec:background}

\subsection{Empirical Dynamical Modeling (EDM)}\label{sec:edm}

Empirical Dynamical Modeling (EDM) is a non-linear time series analysis
framework based on the Takens' embedding theorem~\cite{Deyle2011}. Takens'
theorem states that given a time-series observation of a deterministic
dynamical system, one can reconstruct the underlying attractor manifold of the
dynamical system using time-delayed embeddings of the observation. While the
reconstructed manifold might not preserve the global structure of the original
manifold, it preserves the local topological features (\textit{i.e.} a
diffeomorphism).

Convergent Cross Mapping (CCM)~\cite{Sugihara2012} is one of the EDM methods
that identifies and quantifies the causal relationship between two time series
variables. CCM can identify causality even in the complete absence of
correlation. Figure~\ref{fig:edm} shows the overview of CCM\@. Following is a
brief overview of how CCM works. Suppose two time series $X(t)$ and $Y(t)$ are
given.

\begin{enumerate}
    \item Time series $X$ and $Y$ are embedded into $E$-dimensional space
        using their time lags: Embedding of $X$ is $x$ where $x(t)=\left(X(t),
        X(t-\tau), \dots, X(t-(E-1) \tau)\right)$. $\tau$ is the time lag.
    \item For each point $x(t)$ in $x$, $E+1$ nearest neighbors of $x(t)$ in
        the state space are found. We refer to the nearest neighbors as
        $x(t_1), x(t_2), \dots, x(t_{E+1})$ and the Euclidean distance between
        $x(t)$ and $x(t_i)$ as $d(t, t_i)=\lVert x(t) - x(t_i) \rVert$.
    \item Prediction $\hat{y}(t)$ for $y(t)$ is a linear combination of
        the corresponding points $y(t_1), y(t_2), \dots, y(t_{E+1})$.
        Specifically,
        \begin{equation}
            \hat{y}(t) = \sum^{E+1}_{i=1} \frac{\exp{d(t_i)}}{\sum^{E+1}_{i=1}
            \exp{d(t_i)}} \cdot y(t_i)
        \end{equation}
        The first component of $\hat{y}(t)$ is $\hat{Y}(t)$.
    \item Pearson's correlation $\rho$ between $Y$ and $\hat{Y}$ is computed
        to assess the predictive skill. If $\rho$ is high, we conclude $Y$
        ``CCM-causes'' $X$.
\end{enumerate}

In practice, the $k$-

~\cite{Natsukawa2017,VanBerkel2020}

% cite papers on EDM acceleration
~\cite{Pu2019,Ma2017}.

\subsection{Challenges in mpEDM}\label{sec:challenges}

mpEDM\footnote{\url{https://github.com/keichi/mpEDM}}
This subsection discusses the two major challenges in mpEDM: performance
portability and flexibility.

\subsubsection{Performance portability}

mpEDM was based on ArrayFire~\cite{Malcolm2012}, a general purpose library for
GPU computing. We chose ArrayFire primarily for its productivity and
portability. ArrayFire provides CUDA and OpenCL backends to run on OpenCL
devices. It also provides a CPU backend. Unfortunately, most of the functions
provided by the CPU backend are neither multi-threaded nor vectorized. For this
reason, we implemented our implementation for CPU\@.

As a result, mpEDM had a ArrayFire-based GPU implementation and a CPU-based
implementation of EDM, doubling the maintenance cost. Considering the
diversifying target hardware, we wish a unified implementation that achieves
consistent and reasonable performance across a diverse set of hardware.

\subsubsection{Flexibility}

Since mpEDM was relying on ArrayFire's k-nearest neighbor search function
(\texttt{nearestNeighbour()}), we were unable to modify nor tune the k-NN
search to suit our use case and missed opportunities for further optimization.
ArrayFire's nearest neighbor function accepts two input arrays, which are the
reference and query points, and returns a list of closest reference points and
their distances for each query point. This interface is simple and
easy-to-use. However, when applying to EDM, the time-delayed embedding needs
to be performed on CPU in advance and passed to ArrayFire. This hinders
performance because it increases the amount of data that needs to be read from
GPU memory.

Another potential optimization opportunity is the partial sort function
\texttt{topk()} invoked in the k-NN search. ArrayFire uses NVIDIA's
CUB\footnote{\url{https://nvlabs.github.io/cub/}} library to implement partial
sort. CUB is a highly optimized library and being used by other libraries such
as Thrust\footnote{\url{https://github.com/NVIDIA/thrust}}. ArrayFire's
\texttt{topk()} function divides the input array into equal sized sub-array
and then calls CUB's \texttt{BlockRadixSort()} function to sort each
sub-array. It then extracts the top-$k$ elements from each sub-array and
concatenates them into a new array. This is recursively repeated until the
global top-$k$ elements are found. Even though this implementation is
well-optimized, it may not be optimal for EDM since the targeted $k$ is
relatively small ($\leq 20$).

Finally, we were unable to implement lookups efficiently on GPU using
ArrayFire. ArrayFire provides a construct called \textit{GFOR} that allows one
to perform embarrassingly parallel for-loops in parallel. Although we were
able to implement lookups using GFOR, the attained performance was poor. It
was also challenging to manage memory consumption and memory copies because
ArrayFire implicitly allocates, deallocates and copies arrays.

\section{kEDM}\label{sec:proposal}

\subsection{Overall Design}

kEDM\footnote{\url{https://github.com/keichi/kEDM}} is a performance-portable
implementation of EDM based on Kokkos. To ensure the correctness of the
implementation, the output is validated against the output from mpEDM as well
as the reference implementation pyEDM using automated unit tests.

\subsection{Kokkos}

Kokkos~\cite{Edwards2014} is a performance portability framework developed at
the Sandia National Laboratories. It abstracts away the differences between
low-level programming models such as OpenMP, CUDA, ROCm and exposes a
high-level C++ interface to the developer. Kokkos allows developers to build a
cross-platform and performance-portable application on a single source code
base.

In the Kokkos programming model, the developer specifies (1) the parallel
pattern, (2) computational body, and (3) execution policy of a loop. Available
parallel patterns include parallel-for, parallel-reduce and parallel-scan. The
computational body of a loop is given as a C++ 11 lambda function.

The execution policy defines how a loop is executed. \texttt{RangePolicy}
defines a 1D range of indices. \texttt{TeamPolicy} and \texttt{TeamThreadRange}
is used to coordinate teams of threads to exploit the hierarchical parallelism
of the hardware. On a GPU, teams and threads map to thread blocks and threads
within thread blocks, respectively. On a CPU, teams map to physical cores and
threads map to hardware threads within cores. \texttt{TeamPolicy} gives access
to team-private and thread-private scratch memory, an abstraction of shared
memory in GPUs. Each execution policy is bound to an execution space, an
abstraction of where the code runs. The latest release of Kokkos supports
Serial, OpenMP, OpenMP Target, CUDA, ROCm, Pthread, HIP and HPX\@. A SYCL
backend is also in development.

\textit{View} is the fundamental data type in Kokkos and represents a
multidimensional array. Views are allocated explicitly by the user and
deallocated automatically by Kokkos using reference counting.
Each view is associated to a memory space, an abstraction of
where the data resides.

Listing~\ref{lst:basic} shows a vector addition  kernel implemented in Kokkos.
In this example, a parallel-for loop is launched that iterates over the 1D
range $1..N$.

\begin{lstlisting}[caption={Basic data parallel loop},label={lst:basic}]
Kokkos::parallel_for(RangePolicy<ExecSpace>(N),
KOKKOS_LAMBDA(int i) {
    y(i) = a * x(i) + y(i);
});
\end{lstlisting}

Listing~\ref{lst:hierarchical} shows a matrix vector multiplication kernel
using hierarchical parallelism. The outer parallel-for loop launches $M$ teams
that each computes one row of the output vector $y$. The inner parallel-reduce
computes the dot product of one row in $A$ and $x$.

\begin{lstlisting}[caption={Hierarchical data parallel loop},label={lst:hierarchical}]
parallel_for(TeamPolicy<ExecSpace>(M, AUTO),
KOKKOS_LAMBDA(const member_type &member) {
    int i = member.team_rank();
    float sum = 0.0f;

    parallel_reduce(TeamThreadRange(member, N),
    [=] (int j, float &tmp) {
        tmp += A(i, j) * x(j)
    }, sum);

    single(PerTeam(member),
    [=] () {
        y(i) = sum;
    });
});
\end{lstlisting}

Prior to implementing kEDM, we have examined a number of popular performance
portability frameworks. These include OpenMP, OpenACC, OpenCL and SYCL\@. We
chose Kokkos  because recent studies~\cite{Martineau2017, Deakin2019, Deakin2020}
have shown that it delivers portable performance on a large set of devices
compared to its alternatives. In addition, it has already been adopted by
multiple production applications~\cite{Sprague2020,Holmen2017,Demeshko2019}.
SYCL and OpenMP are attractive choices as they have grown rapidly over the
last few years in terms of hardware coverage and delivered performance, but we
still consider them immature compared to Kokkos at the point of writing this
paper.

\subsection{All k-Nearest Neighbor Search}

We implement the All k-NN search using an exhaustive approach. That is, we
first calculate the pairwise distances between all embedded library points in
the state space and obtain an all-to-all distance matrix. Subsequently, each
row of the obtained distance matrix is partially sorted to find the distances
and indices of the points
to~\cite{Garcia2008,Garcia2010}.

\cite{Johnson2019}
\cite{Shanbhag2018}

\subsubsection{Pairwise distance kernel}
As discussed in Section~\ref{sec:challenges}, performing the time-delayed
embedding into state space and then calculating the pairwise distances is
inefficient since it increases memory access. Instead, we perform the time
delayed embedding and distance calculation at the same time.

Algorithm~\ref{alg:distances} shows the pairwise distance calculation
algorithm in kEDM\@. The outer-most $i$-loop is mapped to teams while the
$j$-loop is mapped to threads within a team. A consideration on CPU is which
loop to vectorize. Since the inner-most $k$-loop is short ($E \leq 20$) in our
use case, vectorizing it is not profitable. We therefore use \textit{SIMD
types}\footnote{\url{https://github.com/Kokkos/simd-math}} to vectorize the
$j$-loop. SIMD types are short vector with overloaded operators that map to
intrinsic functions. SIMD types are mapped to scalars on GPUs and do not cause
any overhead.

Note that the library time series is reused if $E > 1$ and we can expect
more reuse with larger $E$. In addition, we explicitly cache $\mathrm{library}
(k \tau + i)$ (where $k=1 \dots E$) on team-local scratch memory.
% memory alignment

\begin{algorithm}
    \SetAlgoLined
    \DontPrintSemicolon
    \KwIn{Library time series $x$, embedding dimension $E$, time lag $\tau$}
    \KwOut{Pairwise distance matrix $D$}
    \tcp{\texttt{TeamPolicy}}
    \PFor{$i \leftarrow 1$ \KwTo $L$}{
        \tcp{\texttt{ThreadRange}}
        \PFor{$j \leftarrow 1$ \KwTo $L$}{
            $D(i, j) \leftarrow 0$\;
            \For{$k \leftarrow 1$ \KwTo $E$}{
                $D(i, j) \leftarrow D(i, j) + (x(k \tau + i) - x(k \tau + j))^2$\;
            }
        }
    }
    \caption{Pairwise distances}%
    \label{alg:distances}
\end{algorithm}

\subsubsection{Top-$k$ kernel}

The top-$k$ kernel is particularly challenging to implement in Kokkos, because

\begin{itemize}
\item Sort each row of the pairwise distance matrix in parallel. (Each team sorts one row. Threads within a team each finds the top-k elements from a chunk of a row).
\item Use modified insertion sort algorithm shown in previous work to find the top-k elements.
\item Top-k elements are held in thread scratch memory for faster update.
\item \textbf{TODO} automatic tuning of team size
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=.8\columnwidth]{figs/sorting}
    \caption{Top-$k$ sorting algorithm}%
    \label{fig:topk}
\end{figure}

\subsection{Lookup}

Given a library and a target time series, suppose the optimal embedding
dimension of the target time series is $E$. We use the $E$-dimensional embedding
of the library to predict the target. To maximize reuse of the lookup table, we
can group the target time series by its optimal embedding dimension.

Algorithm~\ref{alg:lookup} shows the lookup routine. The outer most $i$-loop
iterates over all time series of which optimal embedding dimension is $E$ and
parallelized using \texttt{TeamPolicy}. Each team performs prediction of one
target time series. The $j$-loop is parallelized using \texttt{TeamThreadRange}
and makes predictions of each time point within a time series (line 5). The
inner most serial $k$-loop is unrolled to exploit ILP\@. Since Kokkos currently
does not have a portable way to request loop unrolling, we use the
\texttt{\#pragma unroll} directive here. The loop body requires indirect access
to the target time series using the indices table. To speed up random access, we
cache the target time series in team-private scratch memory.

Do not write out the predicted time series to global memory and calculate
Pearson’s correlation on-the-fly. Kokkos’ custom reduction feature is used to
implement parallel calculation of correlation coefficient. Algorithm is
based on~\cite{Schubert2018}.

\begin{algorithm}
    \SetAlgoLined
    \DontPrintSemicolon
    \KwIn{Target time series $y$, sorted distance matrix $D$, sorted index
    matrix $T$, embedding dimension $E$}
    \KwOut{Predicted time series $\hat{y}$}
    \PFor{$i \leftarrow 1$ \KwTo $N$}{
        \PFor{$j \leftarrow 1$ \KwTo $L$}{
            \For{$k \leftarrow 1$ \KwTo $E+1$}{
                $\hat{y}(i, j) \leftarrow \hat{y}(i, j) + D(j, k) \cdot y(T(j, k))$ \;
            }
        }
    }
    \caption{Lookup}%
    \label{alg:lookup}
\end{algorithm}

\section{Evaluation}\label{sec:evaluation}

\subsection{Evaluation Environment}

% use table?

We evaluated kEDM on two compute servers installed at the Salk Institute: (1)
Ika and (2) Aori. Ika is equipped with two sockets of 20-core Intel Xeon Gold
6148 CPUs, one NVIDIA V100 PCIe card and 376 GiB of RAM\@. Aori is equipped with
two sockets of 64-core AMD EPYC 7742 CPUs and 1 TiB of RAM\@. kEDM was built
with Kokkos 3.2 on both machines. On Aori, we used AMD Optimizing C/C++ Compiler
(AOCC) 2.2.0, a fork of Clang by AMD. On Ika, we used NVIDIA CUDA Compiler (NVCC)
10.1.

\subsection{Comparison with mpEDM}

\begin{figure}
    \centering
    \includegraphics{figs/breakdown_knn_v100}
    \caption{Breakdown of kNN runtime on V100 ($L=10^4$)}%
    \label{fig:breakdown-knn-v100}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{figs/breakdown_knn_epyc}
    \caption{Breakdown of kNN runtime on EPYC 7742 ($L=10^4$)}%
    \label{fig:breakdown-knn-epyc}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{figs/runtime_lookup_epyc}
    \caption{Runtime of lookups on EPYC 7742 ($L=10^4, N=10^5$)}%
    \label{fig:breakdown-lookup-epyc}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{figs/runtime_lookup_v100}
    \caption{Runtime of lookups on V100 ($L=10^4, N=10^5$)}%
    \label{fig:breakdown-lookup-v100}
\end{figure}

\subsection{Efficiency}

To assess the efficiency of our implementation, we conducted a roofline
analysis~\cite{Williams2008} of our kernels. The compute and memory ceilings
on each platform were measured using the Empirical Roofline Toolkit (ERT)\footnote{\url{https://bitbucket.org/berkeleylab/cs-roofline-toolkit/}} 1.1.0.
Since ERT fails to measure the L1 cache bandwidth on GPUs, we used the
theoretical peak performance instead. We followed the methodology presented
in~\cite{Yang2020a,Yang2020b} to measure the arithmetic intensity and the
attained FLOP/s. Nvprof 10.1 and likwid~\cite{Treibig2010} 5.0.1 were used to
collect the required metrics on GPU and CPU, respectively.

% 1.38*80*32*4/1024 = 13.8 TiB/s
% (frequency)*(# of SMs)*(bank size)*(bank width)

\begin{figure}
    \centering
    \includegraphics{figs/roofline_distances_v100}
    \caption{Roofline analysis of pairwise distance kernel on V100 ($L=10^4$)}%
    \label{fig:roofline-distances-v100}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{figs/roofline_distances_epyc}
    \caption{Roofline analysis of pairwise distance kernel on EPYC 7742 ($L=10^4$)}%
    \label{fig:roofline-distances-epyc}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{figs/roofline_lookup_wo_rho_v100}
    \caption{Roofline analysis of lookup kernel on V100 ($L=10^4, N=10^5$)}%
    \label{fig:roofline-lookup-v100}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{figs/roofline_lookup_wo_rho_epyc}
    \caption{Roofline analysis of lookup kernel on EPYC 7742 ($L=10^4, N=10^5$)}%
    \label{fig:roofline-lookup-eypc}
\end{figure}

\section{Conclusion \& Future Work}\label{sec:conclusion}

Python interface
AMD GPUs and A64FX

\section*{Acknowledgements}
This work was supported by JSPS KAKENHI Grant Number JP20K19808 (KT) and an
Innovation grant by the Kavli Institute for Brain and Mind (GMP).

\clearpage

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
