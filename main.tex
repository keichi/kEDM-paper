\documentclass[conference]{IEEEtran}

\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{cite}
\usepackage[bottom]{footmisc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{multicol}
\usepackage{newtxtext}
\usepackage{newtxmath}
\usepackage{textgreek}
\usepackage{url}
\RequirePackage[l2tabu, orthodox]{nag}

% Allow PDF 1.7 documents to be included with \includegraphics
\pdfminorversion=7

% listing
\lstset{%
  language={C},
  basicstyle={\small\ttfamily},%
  identifierstyle={\small\ttfamily},%
  commentstyle={\small\itshape},%
  keywordstyle={\small\bfseries},%
  ndkeywordstyle={\small\ttfamily},%
  stringstyle={\small\ttfamily},%
  frame={tb},%
  breaklines=true,%
  columns=[l]{fullflexible},%
  numbers=left,%
  numberstyle={\scriptsize},%
  stepnumber=1,%
  numbersep=1em,%
  lineskip=-0.5ex,%
  mathescape,%
  xleftmargin=2em,%
  framexleftmargin=1.5em,%
}

\begin{document}

\title{kEDM: A Performance-portable Implementation of Empirical Dynamical Modeling}

\author{%
    \IEEEauthorblockN{%
        Keichi Takahashi,\\ Wassapon Watanakeesuntorn,\\Kohei Ichikawa
    } \\
    \IEEEauthorblockA{%
        Nara Institute of Science and Technology\\
        Nara, Japan\\
        \{keichi, wassapon.watanakeesuntorn.wq0,\\ ichikawa\}@is.naist.jp
    }
    \and
    \IEEEauthorblockN{%
        George Sugihara
    } \\
    \IEEEauthorblockA{%
        University of California San Diego\\
        California, USA\\
        gsugihara@ucsd.edu
    }
    \and
    \IEEEauthorblockN{%
        Gerald M. Pao
    } \\
    \IEEEauthorblockA{%
        Salk Institute for Biological Studies\\
        California, USA\\
        pao@salk.edu
    }
}

\maketitle

\begin{abstract}
    Recent rapid scale out of high performance computing systems has
    rapidly and continuously increased the scale and complexity of the
    interconnects. As a result, current static and over-provisioned
    interconnects are becoming cost-ineffective. Against this background, we have
    been working on the integration of network programmability into
    the interconnect control, based on the idea that dynamically controlling
    the packet flow in the interconnect according to the communication pattern
    of applications can increase the utilization of interconnects and improve
    application performance. Interconnect simulators come in handy especially
    when investigating the performance characteristics of interconnects with
    different topologies and parameters. However, little effort has been put
    towards the simulation of packet flow in dynamically controlled interconnects,
    while simulators for static interconnects have been extensively researched
    and developed. To facilitate analysis on the performance
    characteristics of dynamic interconnects, we have developed PFAnalyzer.
    PFAnalyzer is a toolset composed of PFSim, an interconnect simulator
    specialized for dynamic interconnects, and PFProf, a profiler.
    PFSim allows interconnect researchers and designers to investigate
    congestion in the interconnect for an arbitrary cluster configuration and
    a set of communication patterns collected by PFProf. PFAnalyzer is used
    to demonstrate how dynamically controlling the interconnects can reduce
    congestion and potentially improve the performance of applications.
\end{abstract}

\begin{IEEEkeywords}
    Simulation, Profiling, Interconnect, Message Passing Interface, Software
    Defined Networking
\end{IEEEkeywords}

\section{Introduction}

Empirical Dynamical Modeling (EDM)~\cite{Chang2017} is a...

\cite{Ma2017}

brain science~\cite{Natsukawa2017}

We have been developing mpEDM~\cite{mpedm}, a massively parallel
implementation of EDM.

\section{Background}

Two major problems with mpEDM: (performance) portability and flexibility.

\subsection{Performance portability}

ArrayFire's CPU implementation is not multi threaded.

Hence we used ArrayFire's implementation on GPU and our implementation on CPU.

\subsection{Flexibility}

Could not modify or tune kNN kernel. For example, embedding on-the-fly,
partial sorting algorithm optimized for small k.

Could not port lookups on GPU.

Create a performance-portable implementation of EDM for both CPU and GPU
platforms.

\begin{itemize}
\item Embedding needs to be done on the CPU which increases memory copies and degrades cache efficiency
\item kNN kernel is not optimized for all kNN, relatively small number of
    reference points, small k. ArrayFire’s kNN performs radix sort recursively
\item ArrayFire’s CPU implementation of kNN search is not multithreaded. We
    had a handwritten kNN kernel for CPU. Having different implementations for
    CPU and GPU makes the maintenance cost higher
\item Lookup function was not implemented on GPU (tried ArrayFire’s GFOR loops
    but the performance was low)
\end{itemize}

\begin{algorithm}
    \SetAlgoLined
    \DontPrintSemicolon
    \KwIn{$library$ and $target$ time series, embedding dimension $E$, time lag \texttau}
    \KwOut{Arrays $diatances$ and $indices$ for lookup}
    \For{$i \leftarrow 1$ \KwTo $L$}{
        $distances(i, :) \leftarrow 0$\;
        \For{$j \leftarrow 1$ \KwTo $L$}{
            \For{$k \leftarrow 1$ \KwTo $E$}{
                $indices(i, j) \leftarrow j$\;
                $distances(i, j) \leftarrow distances(i, j) + (target(k $\texttau$ + i) - library(k $\texttau$ + j))^2$\;
            }
        }
    }
    \caption{Pairwise distances}
    \label{pseudo:knn_cpu}
\end{algorithm}

\section{kEDM}

\subsection{Overall Design}

kEDM\footnote{\url{https://github.com/keichi/kEDM}} is a new implementation
based on Kokkos.

What is Kokkos?
\begin{itemize}
\item A programming model that abstracts
\item Views: Multi-dimensional arrays with architecture-dependent layouts
\item Execution polices (parallel patterns): for, reduce
\item Hierarchical parallelism: Allows one to exploit the hierarchy in a
    platform. Additionally, provides an interface to use high-speed
    team/thread scratch memory.
\end{itemize}

Why Kokkos?
\begin{itemize}
    \item OpenMP and OpenACC need compiler support
    \item Kokkos achieves performance close to native programming model (e.g. CUDA)
    \item Rich ecosystem including various profilers (kooks-tools) and reference kernels  (Kokkos-kernels)
    \item Participates in the ISO CPP standardization
\end{itemize}

Reduce CPU-GPU copies as much as possible. CPU-CPU copy happens only before
and after the cross mapping

\begin{figure*}
    \centering
    \includegraphics{figs/xmap_overview}
    \caption{Overview of EDM cross mapping}%
    \label{fig:architecture}
\end{figure*}

\subsection{All k-Nearest Neighbors Kernel}

NVIDIA CUB\footnote{\url{https://nvlabs.github.io/cub/}}

Pairwise distance calculation:
\begin{itemize}
\item Perform time delayed embedding while calculating the pairwise distances
    to reduce memory copy between CPU and GPU and improve cache hit.
\item Use team scratch memory to cache one of the time series.
\item \textbf{TODO} Loop interchange was needed to achieve full vectorization
    on CPU (separate kernel for CPU and GPU)
\end{itemize}

Partial sort:
\begin{itemize}
\item Sort each row of the pairwise distance matrix in parallel. (Each team sorts one row. Threads within a team each finds the top-k elements from a chunk of a row).
\item Use modified insertion sort algorithm shown in previous work to find the top-k elements.
\item Top-k elements are held in thread scratch memory for faster update.
\item \textbf{TODO} automatic tuning of team size
\end{itemize}

\subsection{Lookup kernel}

\begin{itemize}
\item Perform many lookups in parallel. (Each team performs lookups from one
    source time series)
\item Cache target time series in team scratch memory for faster random
    access.
\item Do not write out the predicted time series to global memory and
    calculate Pearson’s correlation on-the-fly. Kokkos’ custom reduction
    feature is used to implement parallel calculation of correlation
    coefficient. Algorithm is based on~\cite{Schubert2018}
\end{itemize}

\section{Evaluation}

\subsection{Evaluation Environment}

% use table?

We evaluated kEDM on two compute servers installed at the Salk Institute: (1)
Ika and (2) Aori.
Ika is equipped with two sockets of 20-core Intel Xeon Gold 6148 CPUs, one
NVIDIA V100 PCIe card and 376 GiB of RAM. Aori is equipped with two sockets of
64-core AMD EPYC 7742 CPUs and 1 TiB of RAM.
kEDM was built with Kokkos 3.2.00 on both machines. On Aori, we used AMD
Optimizing C/C++ Compiler (AOCC) 2.2.0, a fork of Clang by AMD. On Ika, we
used NVIDIA CUDA Compiler (NVCC) 10.1.

\subsection{Comparison with mpEDM}

\begin{figure}
    \centering
    \includegraphics{figs/breakdown_knn_v100}
    \caption{Breakdown of kNN runtime on V100}%
    \label{fig:architecture}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{figs/breakdown_knn_epyc}
    \caption{Breakdown of kNN runtime on EPYC 7742}%
    \label{fig:architecture}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{figs/runtime_lookup_epyc}
    \caption{Breakdown of lookup runtime on EPYC 7742}%
    \label{fig:architecture}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{figs/runtime_lookup_v100}
    \caption{Breakdown of lookup runtime on V100}%
    \label{fig:architecture}
\end{figure}

\subsection{Efficiency}

To assess the efficiency of our implementation, we conducted a roofline
analysis~\cite{Williams2008} of our kernels. The compute and memory ceilings
on each platform were measured using the Empirical Roofline Toolkit (ERT)\footnote{\url{https://bitbucket.org/berkeleylab/cs-roofline-toolkit/}} 1.1.0.
Since ERT fails to measure the L1 cache bandwidth on GPUs, we used the
theoretical peak performance instead. We followed the methodology presented
in~\cite{Yang2020a,Yang2020b} to measure the arithmetic intensity and the
attained FLOP/s. Nvprof 10.1 and likwid~\cite{Treibig2010} 5.0.1 were used to
collect the required metrics on GPU and CPU, respectively.

% 1.38*80*32*4/1024 = 13.8 TiB/s
% (frequency)*(# of SMs)*(bank size)*(bank width)

\begin{figure}
    \centering
    \includegraphics{figs/roofline_distances_v100}
    \caption{Roofline analysis of pairwise distance kernel on V100}%
    \label{fig:architecture}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{figs/roofline_distances_epyc}
    \caption{Roofline analysis of pairwise distance kernel on EPYC 7742}%
    \label{fig:architecture}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{figs/roofline_lookup_v100}
    \caption{Roofline analysis of lookup kernel on V100}%
    \label{fig:architecture}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{figs/roofline_lookup_epyc}
    \caption{Roofline analysis of lookup kernel on EPYC 7742}%
    \label{fig:architecture}
\end{figure}

\section{Conclusion \& Future Work}

\section*{Acknowledgements}
This work was supported by JSPS KAKENHI Grant Number JP20K19808.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
